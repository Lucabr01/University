## Table of Contents

1. [Introduction](#1-introduction)
2. [Soft-Actor-Critic based model](#2-soft-actor-critic-based-model)
   1. [Environment description](#21-environment-description)
   2. [Custom Reward Function](#22-custom-reward-function)
   3. [Training](#23-training)
   4. [Results](#24-results)
3. [Evolutionary Strategies](#3-evolutionary-strategies)

      
# 1. Introduction

Data centers are among the most power-consuming infrastructures worldwide, and a substantial portion of their total energy usage is devoted to cooling. Server reliability depends on maintaining safe thermal conditions, yet traditional static or rule-based cooling strategies often lead to severe energy inefficiencies.   This project addresses the Data Center Cooling Optimization Problem through intelligent control techniques based on Reinforcement Learning.

The entire work is carried out in a Sinergym-based environment.  
Sinergym [ref] is a Python framework that integrates EnergyPlus [ref], a building-level simulation engine, with the Gymnasium API. This makes it possible to control realistic HVAC (**Heating, Ventilation & Air Conditioning**) systems using RL agents.

In simple terms, the agent learns how to reduce unnecessary energy consumption without letting temperatures exceed safe limits for IT equipment.

Formally, the two main goals of the agent are:

- **Maintaining thermal comfort:** keeping the internal air temperature of the datacenter within the optimal comfort range of **18°C–27°C** (ASHRAE recommended level [ref]).  
- **Reducing energy consumption:** minimizing the total **HVAC electricity demand** compared to a static baseline (e.g., fixed setpoint at 21.5°C), while still maintaining acceptable comfort.

To explore different learning paradigms, the project evaluates **two control approaches**:  
a model-free **Soft-Actor-Critic (SAC)** agent and a gradient-free **Evolutionary Strategies (ES)** agent.  
Both aim to find the optimal balance between comfort and energy efficiency, maximizing overall performance.

# 2. Soft-Actor-Critic based model

This section presents the architecture and training procedure of the SAC agent designed for datacenter cooling control. After defining the environment and the observation–action structure, we describe the custom reward function used to balance thermal comfort and energy efficiency. Finally, we detail the training configuration and report the performance metrics obtained during evaluation.

The choice of SAC is not arbitrary. Prior work, including the study by **Bienmann et al.** [ref], shows that SAC provides **faster and more stable convergence** than other continuous-control RL algorithms in datacenter cooling tasks. This makes it a strong baseline for comparison with alternative methods such as ES.

## 2.1 Environment description

The training environment is the `Eplus-datacenter_dx-mixed-continuous-v1` model provided by Sinergym.  
It represents a **two-zone datacenter** equipped with an HVAC system controlled by a CRAC unit (Computer Room Air Conditioner).  
The simulation runs on **EnergyPlus**, using the **New York (USA) weather file**, which makes the task particularly challenging due to the strong seasonal variability: the climate alternates between very hot summers and extremely cold winters, forcing the cooling system to adapt to rapidly changing thermal loads.

### Datacenter Structure
The building model consists of:
- **East Zone** – server room area with heat-generating equipment  
- **West Zone** – second server room with similar load characteristics  

Both zones contribute to the overall thermal dynamics, and the agent must ensure that neither exceeds the safe operating temperature threshold.

### CRAC System and Cooling Control
The HVAC system is driven by a **CRAC (Computer Room Air Conditioner)** unit.  
A CRAC is a specialized cooling device designed for datacenters and network rooms. It continuously:
- monitors temperature and humidity,  
- removes heat generated by servers,  
- distributes cooled air to maintain stable conditions.

In this environment, the RL agent directly controls the **Cooling Return Air Setpoint**, which defines how cold the air should be before being supplied back to the server racks.  
This variable is critical:

- If the setpoint is **too low**, the system overcools the air and consumes excessive electricity.  
- If the setpoint is **too high**, the servers may operate above the recommended thermal range, risking overheating.

Therefore, adjusting this setpoint is a central part of balancing **thermal comfort vs. energy efficiency**.

## 2.2 Custom Reward Function

The reward function is based on the formulation proposed in the reference paper, but we omit all occupancy-related components.  
Datacenters must operate **24/7**, and unlike office buildings there is no concept of human presence: the thermal comfort constraints are tied exclusively to the safe operating range of **IT equipment**.  
For this reason, the reward focuses only on:
- keeping temperatures within the recommended comfort range,
- reducing HVAC energy consumption.

### Conceptual Description
The reward penalizes two quantities:
1. **Energy consumption**: higher HVAC electricity demand results in a higher penalty.  
2. **Thermal violations**: temperatures above the allowed threshold incur an exponential penalty that grows rapidly as the datacenter approaches unsafe thermal conditions.

If the agent maintains temperatures inside the comfort zone while keeping energy demand low, the penalty is small (reward close to zero).  
When temperatures exceed safe limits or the HVAC system works inefficiently, the penalty increases significantly.

### Formal Definition
For each timestep $t$, the reward has the general form:

$$
r_t = - w_E \cdot P^{(E)}_t - w_T \cdot P^{(T)}_t
$$

where:
- $w_E$ is the weight of the energy penalty,  
- $w_T$ is the weight of the thermal penalty,  
- $P^{(E)}_t$ is the normalized energy penalty,  
- $P^{(T)}_t$ is the thermal violation penalty.

#### Energy Penalty
Let $E_t$ be the HVAC electricity demand at time $t$.  
The normalized energy penalty is:

$$
P^{(E)}_t = \frac{E_t}{\mathrm{energy\ scale}}
$$

The scaling factor ensures that the magnitude of the energy cost remains comparable to thermal costs.

#### Thermal Penalty

Let $T_t$ be the maximum temperature among all datacenter zones at time $t$.  
The thermal penalty is defined in three segments:

1. **Comfort Zone**
   
   $$T_t \leq T_{\text{high}} \qquad \Rightarrow \qquad P^{(T)}_t = 0$$

2. **Warning Zone**

$$
T_{\text{high}} < T_t < T_{\text{red}}
$$

$$
P^{(T)}_t = \exp\big(\alpha (T_t - T_{\text{high}})\big) - 1
$$

3. **Red Zone**

$$
T_t \geq T_{\text{red}}
$$

$$
P^{(T)}_t = C_{\text{AL}} + \exp\big(\beta (T_t - T_{\text{red}})\big) - 1
$$


The constant $C_{\text{AL}}$ ensures continuity between the warning and red regions:

$$C_{\text{AL}} = \exp\left[\alpha (T_{\text{red}} - T_{\text{high}})\right] - 1$$

This formulation captures the nonlinear nature of thermal violations inside a datacenter:  
temperatures slightly above the comfort threshold are tolerable for brief periods, but values approaching overheating become exponentially dangerous.
### Interpretation
- In **safe conditions**, the reward is mainly driven by energy usage.  
- In **marginal conditions**, the warning-zone exponential term grows and encourages conservative behavior.  
- In **unsafe conditions**, the red-zone exponential term dominates, strongly penalizing overheating to prevent equipment damage.

This structure allows the agent to learn a policy that preserves **thermal safety** while minimizing **HVAC electricity demand**, which is essential for datacenter efficiency.

Moreover, the smooth exponential shape of the penalty prevents the agent from being “forced” into a narrow band of setpoints.  
Instead of collapsing onto a single safe-but-wasteful temperature, the agent is encouraged to **explore higher cooling setpoints** when appropriate, because the penalty increases gradually rather than abruptly.  
This makes the reward function well-suited for discovering energy-efficient operating regions that would be missed with harsher or discontinuous formulations.

The complete Sinergym-frienly implementation of this reward function is available in the repository inside the file **`Custom_reward.py`**.


## 2.3 Training
## 2.4 Results

# 3. Evolutionary Strategies
